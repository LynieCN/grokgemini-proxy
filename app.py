from fastapi import FastAPI, HTTPException, Request, status, Security, Header
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, validator
import httpx
import asyncio
import logging
import json
import time
import hashlib
from typing import Optional, Dict, Any
from enum import Enum
from collections import defaultdict
from datetime import datetime, timedelta
import uuid

# é…ç½®æ—¥å¿— - ç§»é™¤æ•æ„Ÿä¿¡æ¯
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ğŸ”’ å®‰å…¨é…ç½®
ENABLE_ACCESS_TOKEN = False  # è®¾ç½®ä¸º True å¯ç”¨è®¿é—®ä»¤ç‰ŒéªŒè¯
ACCESS_TOKEN = "your-secret-token-here"  # ä¿®æ”¹ä¸ºä½ çš„è®¿é—®ä»¤ç‰Œ

# ğŸ”’ é€Ÿç‡é™åˆ¶é…ç½®
RATE_LIMIT_REQUESTS = 20  # æ¯ä¸ª IP æ¯åˆ†é’Ÿæœ€å¤šè¯·æ±‚æ•°
RATE_LIMIT_WINDOW = 60  # æ—¶é—´çª—å£ï¼ˆç§’ï¼‰

# ğŸ”’ è¯·æ±‚å¤§å°é™åˆ¶
MAX_MESSAGE_LENGTH = 10000  # å•æ¡æ¶ˆæ¯æœ€å¤§å­—ç¬¦æ•°
MAX_MESSAGES_COUNT = 50  # æœ€å¤§æ¶ˆæ¯æ•°é‡
MAX_TOKENS = 8000  # æœ€å¤§ token æ•°

app = FastAPI(
    title="Grok & Gemini API Proxy (Secured)",
    description="é«˜æ€§èƒ½å®‰å…¨ä»£ç†æ¥å£ï¼ŒOpenAI å…¼å®¹æ ¼å¼",
    version="2.3.0",
    docs_url=None if ENABLE_ACCESS_TOKEN else "/docs",
    redoc_url=None if ENABLE_ACCESS_TOKEN else "/redoc"
)

# ğŸ”’ CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”æ”¹ä¸ºå…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["POST", "GET"],
    allow_headers=["*"],
)

# API åŸºç¡€ URL é…ç½®
API_BASES = {
    "grok": "https://api.x.ai/v1",
    "gemini": "https://generativelanguage.googleapis.com/v1beta"
}

# ğŸ”’ é€Ÿç‡é™åˆ¶å­˜å‚¨ï¼ˆå†…å­˜ï¼‰
rate_limit_storage: Dict[str, list] = defaultdict(list)

# ğŸ”’ å…è®¸çš„æ¨¡å‹åˆ—è¡¨ï¼ˆç™½åå•ï¼‰- ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·è¦æ±‚
ALLOWED_MODELS = {
    "grok": ["grok-4-1-fast-reasoning", "grok-4-1-fast-non-reasoning", "grok-4-0709"],
    "gemini": ["gemini-3-flash-preview", "gemini-3-pro-preview", "gemini-2.5-flash-lite"]
}

class Provider(str, Enum):
    GROK = "grok"
    GEMINI = "gemini"

# åˆ›å»ºå…±äº«çš„ HTTP å®¢æˆ·ç«¯
http_client = httpx.AsyncClient(
    timeout=httpx.Timeout(60.0, connect=10.0),
    limits=httpx.Limits(max_keepalive_connections=20, max_connections=100),
    http2=True
)

class ChatRequest(BaseModel):
    messages: list[dict] = Field(..., description="æ¶ˆæ¯åˆ—è¡¨")
    model: str = Field(..., description="æ¨¡å‹ID")
    max_tokens: Optional[int] = Field(None, ge=1, le=MAX_TOKENS, description="æœ€å¤§ä»¤ç‰Œæ•°")
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    top_p: Optional[float] = Field(None, ge=0.0, le=1.0, description="Top-pé‡‡æ ·")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¾“å‡º")
    presence_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)
    frequency_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)
    provider: Optional[str] = Field(None, description="æŒ‡å®šAPIæä¾›å•†")

    @validator('messages')
    def validate_messages(cls, v):
        """éªŒè¯æ¶ˆæ¯åˆ—è¡¨"""
        if not v:
            raise ValueError("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if len(v) > MAX_MESSAGES_COUNT:
            raise ValueError(f"æ¶ˆæ¯æ•°é‡ä¸èƒ½è¶…è¿‡ {MAX_MESSAGES_COUNT}")
        
        for msg in v:
            if 'role' not in msg or 'content' not in msg:
                raise ValueError("æ¯æ¡æ¶ˆæ¯å¿…é¡»åŒ…å« role å’Œ content")
            
            if msg['role'] not in ['system', 'user', 'assistant']:
                raise ValueError(f"æ— æ•ˆçš„è§’è‰²: {msg['role']}")
            
            if len(str(msg['content'])) > MAX_MESSAGE_LENGTH:
                raise ValueError(f"å•æ¡æ¶ˆæ¯ä¸èƒ½è¶…è¿‡ {MAX_MESSAGE_LENGTH} å­—ç¬¦")
        
        return v
    
    @validator('model')
    def validate_model(cls, v):
        """éªŒè¯æ¨¡å‹åç§°"""
        # æ£€æŸ¥æ˜¯å¦åœ¨å…è®¸çš„æ¨¡å‹åˆ—è¡¨ä¸­
        for provider_models in ALLOWED_MODELS.values():
            if v in provider_models:
                return v
        
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {v}")

def mask_api_key(api_key: str) -> str:
    """ğŸ”’ è„±æ• API Key"""
    if not api_key or len(api_key) < 8:
        return "***"
    return f"{api_key[:4]}...{api_key[-4:]}"

def get_client_ip(request: Request) -> str:
    """è·å–å®¢æˆ·ç«¯çœŸå® IP"""
    forwarded = request.headers.get("X-Forwarded-For")
    if forwarded:
        return forwarded.split(",")[0].strip()
    
    real_ip = request.headers.get("X-Real-IP")
    if real_ip:
        return real_ip
    
    return request.client.host if request.client else "unknown"

def check_rate_limit(ip: str) -> bool:
    """ğŸ”’ æ£€æŸ¥é€Ÿç‡é™åˆ¶"""
    now = time.time()
    
    # æ¸…ç†è¿‡æœŸè®°å½•
    rate_limit_storage[ip] = [
        timestamp for timestamp in rate_limit_storage[ip]
        if now - timestamp < RATE_LIMIT_WINDOW
    ]
    
    # æ£€æŸ¥æ˜¯å¦è¶…é™
    if len(rate_limit_storage[ip]) >= RATE_LIMIT_REQUESTS:
        return False
    
    # è®°å½•æœ¬æ¬¡è¯·æ±‚
    rate_limit_storage[ip].append(now)
    return True

async def verify_access_token(authorization: Optional[str] = Header(None)) -> bool:
    """ğŸ”’ éªŒè¯è®¿é—®ä»¤ç‰Œï¼ˆå¯é€‰ï¼‰"""
    if not ENABLE_ACCESS_TOKEN:
        return True
    
    if not authorization:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="éœ€è¦è®¿é—®ä»¤ç‰Œ"
        )
    
    token = authorization.replace("Bearer ", "")
    if token != ACCESS_TOKEN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="è®¿é—®ä»¤ç‰Œæ— æ•ˆ"
        )
    
    return True

def detect_provider_from_model(model: str) -> str:
    """æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨æ£€æµ‹æä¾›å•†"""
    for provider, models in ALLOWED_MODELS.items():
        if model in models:
            return provider
    
    # åå¤‡æ£€æµ‹
    model_lower = model.lower()
    if any(x in model_lower for x in ["grok"]):
        return "grok"
    elif any(x in model_lower for x in ["gemini"]):
        return "gemini"
    
    raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ¨¡å‹: {model}")

def convert_gemini_to_openai(gemini_response: dict, model: str) -> dict:
    """
    ğŸ”„ å°† Gemini å“åº”è½¬æ¢ä¸º OpenAI æ ¼å¼ï¼ˆéæµå¼ï¼‰
    """
    try:
        # Gemini å“åº”æ ¼å¼ç¤ºä¾‹ï¼š
        # {
        #   "candidates": [{
        #     "content": {
        #       "parts": [{"text": "..."}],
        #       "role": "model"
        #     },
        #     "finishReason": "STOP"
        #   }],
        #   "usageMetadata": {...}
        # }
        
        if "candidates" not in gemini_response or not gemini_response["candidates"]:
            # å¦‚æœæ²¡æœ‰candidatesï¼Œè¿”å›ç©ºå“åº”
            return {
                "id": f"chatcmpl-{uuid.uuid4().hex[:24]}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": ""
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0
                }
            }
        
        candidate = gemini_response["candidates"][0]
        content_parts = candidate.get("content", {}).get("parts", [])
        
        # åˆå¹¶æ‰€æœ‰ text parts
        text_content = "".join(part.get("text", "") for part in content_parts)
        
        # è½¬æ¢ finishReason
        finish_reason_map = {
            "STOP": "stop",
            "MAX_TOKENS": "length",
            "SAFETY": "content_filter",
            "RECITATION": "content_filter",
            "OTHER": "stop"
        }
        finish_reason = finish_reason_map.get(
            candidate.get("finishReason", "STOP"), 
            "stop"
        )
        
        # æå– usage ä¿¡æ¯
        usage_metadata = gemini_response.get("usageMetadata", {})
        
        # æ„é€  OpenAI æ ¼å¼å“åº”
        openai_response = {
            "id": f"chatcmpl-{uuid.uuid4().hex[:24]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": text_content
                },
                "finish_reason": finish_reason
            }],
            "usage": {
                "prompt_tokens": usage_metadata.get("promptTokenCount", 0),
                "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
                "total_tokens": usage_metadata.get("totalTokenCount", 0)
            }
        }
        
        return openai_response
        
    except Exception as e:
        logger.error(f"Gemini å“åº”è½¬æ¢å¤±è´¥: {type(e).__name__}")
        # è¿”å›ä¸€ä¸ªåŸºç¡€çš„ OpenAI æ ¼å¼å“åº”
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:24]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "å“åº”è½¬æ¢é”™è¯¯"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }

def convert_gemini_stream_chunk_to_openai(gemini_chunk: dict, model: str) -> Optional[dict]:
    """
    ğŸ”„ å°† Gemini æµå¼å“åº”å—è½¬æ¢ä¸º OpenAI æ ¼å¼
    """
    try:
        # Gemini æµå¼å“åº”æ ¼å¼ï¼š
        # {
        #   "candidates": [{
        #     "content": {
        #       "parts": [{"text": "..."}],
        #       "role": "model"
        #     }
        #   }]
        # }
        
        if "candidates" not in gemini_chunk or not gemini_chunk["candidates"]:
            return None
        
        candidate = gemini_chunk["candidates"][0]
        content_parts = candidate.get("content", {}).get("parts", [])
        
        # æå–æ–‡æœ¬å†…å®¹
        text_content = "".join(part.get("text", "") for part in content_parts)
        
        if not text_content:
            return None
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸå—
        finish_reason = None
        if "finishReason" in candidate:
            finish_reason_map = {
                "STOP": "stop",
                "MAX_TOKENS": "length",
                "SAFETY": "content_filter",
                "RECITATION": "content_filter",
                "OTHER": "stop"
            }
            finish_reason = finish_reason_map.get(candidate["finishReason"], "stop")
        
        # æ„é€  OpenAI æµå¼æ ¼å¼
        openai_chunk = {
            "id": f"chatcmpl-{uuid.uuid4().hex[:24]}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "content": text_content
                } if text_content else {},
                "finish_reason": finish_reason
            }]
        }
        
        return openai_chunk
        
    except Exception as e:
        logger.error(f"Gemini æµå¼å—è½¬æ¢å¤±è´¥: {type(e).__name__}")
        return None

def build_gemini_payload(req: ChatRequest) -> Dict[str, Any]:
    """æ„å»º Gemini è¯·æ±‚è´Ÿè½½"""
    contents = []
    for msg in req.messages:
        role = "model" if msg["role"] == "assistant" else "user"
        contents.append({
            "role": role,
            "parts": [{"text": msg["content"]}]
        })
    
    payload = {"contents": contents}
    
    generation_config = {}
    if req.temperature is not None:
        generation_config["temperature"] = req.temperature
    if req.max_tokens is not None:
        generation_config["maxOutputTokens"] = req.max_tokens
    if req.top_p is not None:
        generation_config["topP"] = req.top_p
    
    if generation_config:
        payload["generationConfig"] = generation_config
    
    return payload

def build_grok_payload(req: ChatRequest) -> Dict[str, Any]:
    """æ„å»º Grok è¯·æ±‚è´Ÿè½½"""
    payload = {
        "model": req.model,
        "messages": req.messages,
        "stream": req.stream
    }
    
    if req.max_tokens is not None:
        payload["max_tokens"] = req.max_tokens
    if req.temperature is not None:
        payload["temperature"] = req.temperature
    if req.top_p is not None:
        payload["top_p"] = req.top_p
    if req.presence_penalty is not None:
        payload["presence_penalty"] = req.presence_penalty
    if req.frequency_penalty is not None:
        payload["frequency_penalty"] = req.frequency_penalty
    
    return payload

def build_headers(provider: str, api_key: str, stream: bool = False) -> Dict[str, str]:
    """æ„å»ºè¯·æ±‚å¤´"""
    if provider == "gemini":
        return {"Content-Type": "application/json"}
    else:  # grok
        return {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream" if stream else "application/json"
        }

def build_url(provider: str, endpoint: str, api_key: str = None, model: str = None) -> str:
    """æ„å»ºè¯·æ±‚ URL"""
    base_url = API_BASES[provider]
    
    if provider == "gemini":
        if endpoint == "chat":
            return f"{base_url}/models/{model}:generateContent?key={api_key}"
        elif endpoint == "stream":
            return f"{base_url}/models/{model}:streamGenerateContent?key={api_key}&alt=sse"
        else:
            return f"{base_url}/models?key={api_key}"
    else:
        if endpoint == "chat":
            return f"{base_url}/chat/completions"
        else:
            return f"{base_url}/models"

async def stream_gemini_response(response, model: str):
    """
    ğŸ”„ Gemini æµå¼å“åº”å¤„ç† - è½¬æ¢ä¸º OpenAI æ ¼å¼
    """
    try:
        async for line in response.aiter_lines():
            if line:
                if line.startswith("data: "):
                    data_str = line[6:].strip()
                    
                    # è·³è¿‡ç©ºæ•°æ®
                    if not data_str:
                        continue
                    
                    try:
                        # è§£æ Gemini çš„ JSON æ•°æ®
                        gemini_data = json.loads(data_str)
                        
                        # è½¬æ¢ä¸º OpenAI æ ¼å¼
                        openai_chunk = convert_gemini_stream_chunk_to_openai(gemini_data, model)
                        
                        if openai_chunk:
                            # æŒ‰ç…§ SSE æ ‡å‡†æ ¼å¼è¾“å‡ºï¼šdata: {json}\n\n
                            yield f"data: {json.dumps(openai_chunk)}\n\n"
                        
                    except json.JSONDecodeError:
                        # å¦‚æœä¸æ˜¯ JSONï¼Œè·³è¿‡
                        continue
        
        # å‘é€ç»“æŸæ ‡è®°ï¼ˆOpenAI æ ¼å¼ï¼‰
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"Gemini æµå¼ä¼ è¾“é”™è¯¯: {type(e).__name__}")
        error_chunk = {
            "id": f"chatcmpl-{uuid.uuid4().hex[:24]}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"
        yield "data: [DONE]\n\n"

async def stream_grok_response(response):
    """
    ğŸ“¡ Grok æµå¼å“åº”å¤„ç† - ç¡®ä¿ç¬¦åˆ SSE æ ‡å‡†æ ¼å¼
    """
    try:
        async for line in response.aiter_lines():
            if line:
                # Grok å·²ç»æ˜¯ OpenAI æ ¼å¼ï¼Œä½†éœ€è¦ç¡®ä¿æ ¼å¼è§„èŒƒ
                if line.startswith("data: "):
                    # æŒ‰ç…§ SSE æ ‡å‡†ï¼šæ¯ä¸ªæ•°æ®å—åé¢å¿…é¡»æœ‰ä¸¤ä¸ªæ¢è¡Œç¬¦
                    yield f"{line}\n\n"
                else:
                    # å¦‚æœä¸æ˜¯æ ‡å‡†æ ¼å¼ï¼Œä¿®æ­£å®ƒ
                    yield f"data: {line}\n\n"
                    
    except Exception as e:
        logger.error(f"Grok æµå¼ä¼ è¾“é”™è¯¯: {type(e).__name__}")
        error_data = json.dumps({'error': 'æµå¼ä¼ è¾“é”™è¯¯'})
        yield f"data: {error_data}\n\n"

@app.post("/v1/chat/completions")
@app.post("/{provider}/v1/chat/completions")
async def chat_completions(
    req: ChatRequest,
    request: Request,
    provider: str = None,
    authorization: Optional[str] = Header(None)
):
    """èŠå¤©å®Œæˆæ¥å£ - OpenAI å…¼å®¹æ ¼å¼"""
    
    # ğŸ”’ è®¿é—®ä»¤ç‰ŒéªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if ENABLE_ACCESS_TOKEN:
        await verify_access_token(authorization)
    
    # ğŸ”’ é€Ÿç‡é™åˆ¶æ£€æŸ¥
    client_ip = get_client_ip(request)
    if not check_rate_limit(client_ip):
        logger.warning(f"é€Ÿç‡é™åˆ¶è§¦å‘: IP={client_ip}")
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail=f"è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·åœ¨ {RATE_LIMIT_WINDOW} ç§’åé‡è¯•"
        )
    
    # æ£€æµ‹æä¾›å•†
    if provider is None:
        provider = req.provider or detect_provider_from_model(req.model)
    
    provider = provider.lower()
    
    if provider not in API_BASES:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}"
        )
    
    # ğŸ”’ éªŒè¯æ¨¡å‹åç§°
    if req.model not in ALLOWED_MODELS.get(provider, []):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"æ¨¡å‹ {req.model} ä¸æ”¯æŒæä¾›å•† {provider}"
        )
    
    # è·å– API Key
    api_key = request.headers.get("Authorization", "").replace("Bearer ", "")
    if not api_key and provider == "gemini":
        api_key = request.headers.get("x-api-key", "")
    
    if not api_key:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ç¼ºå°‘APIå¯†é’¥"
        )
    
    # ğŸ”’ è®°å½•è¯·æ±‚ï¼ˆè„±æ•ï¼‰
    logger.info(
        f"è¯·æ±‚: IP={client_ip} Provider={provider} Model={req.model} "
        f"Stream={req.stream} APIKey={mask_api_key(api_key)}"
    )

    try:
        # æ„å»ºè¯·æ±‚
        headers = build_headers(provider, api_key, req.stream)
        
        if provider == "gemini":
            payload = build_gemini_payload(req)
            if req.stream:
                url = build_url(provider, "stream", api_key, req.model)
            else:
                url = build_url(provider, "chat", api_key, req.model)
        else:
            payload = build_grok_payload(req)
            url = build_url(provider, "chat", api_key)
        
        # ğŸ”’ ä¸è®°å½•å®Œæ•´ URLï¼ˆGemini URL åŒ…å« API keyï¼‰
        if provider == "gemini":
            logger.debug(f"è¯·æ±‚ Gemini API")
        else:
            logger.debug(f"è¯·æ±‚ {provider} API")
        
        # å‘é€è¯·æ±‚
        response = await http_client.post(
            url,
            headers=headers,
            json=payload,
            timeout=60.0
        )
        response.raise_for_status()

        # è¿”å›å“åº”
        if req.stream:
            if provider == "gemini":
                # Gemini æµå¼ â†’ OpenAI æ ¼å¼
                return StreamingResponse(
                    stream_gemini_response(response, req.model),
                    media_type="text/event-stream"
                )
            else:
                # Grok æµå¼ â†’ è§„èŒƒåŒ– SSE æ ¼å¼
                return StreamingResponse(
                    stream_grok_response(response),
                    media_type="text/event-stream"
                )
        else:
            # éæµå¼å“åº”
            response_data = response.json()
            
            if provider == "gemini":
                # Gemini éæµå¼ â†’ OpenAI æ ¼å¼
                return convert_gemini_to_openai(response_data, req.model)
            else:
                # Grok å·²ç»æ˜¯ OpenAI æ ¼å¼
                return response_data

    except httpx.HTTPStatusError as e:
        status_code = e.response.status_code
        
        # ğŸ”’ å®‰å…¨çš„é”™è¯¯å¤„ç†
        if status_code == 401:
            error_msg = "API å¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ"
        elif status_code == 429:
            error_msg = "API é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•"
        elif status_code == 500:
            error_msg = "ä¸Šæ¸¸æœåŠ¡å™¨é”™è¯¯"
        else:
            error_msg = f"è¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {status_code})"
        
        logger.error(f"{provider} API é”™è¯¯: {status_code}")
        raise HTTPException(status_code=status_code, detail=error_msg)
        
    except httpx.TimeoutException:
        logger.error(f"{provider} API è¶…æ—¶")
        raise HTTPException(
            status_code=504,
            detail="è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        )
    except Exception as e:
        logger.error(f"æœªé¢„æœŸé”™è¯¯: {type(e).__name__}")
        raise HTTPException(
            status_code=500,
            detail="æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
        )

@app.get("/v1/models")
@app.get("/{provider}/v1/models")
async def get_models(request: Request, provider: str = None):
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ - OpenAI å…¼å®¹æ ¼å¼"""
    
    # ğŸ”’ é€Ÿç‡é™åˆ¶
    client_ip = get_client_ip(request)
    if not check_rate_limit(client_ip):
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail="è¯·æ±‚è¿‡äºé¢‘ç¹"
        )
    
    if provider:
        provider = provider.lower()
        if provider not in API_BASES:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")
        
        # è¿”å› OpenAI æ ¼å¼çš„æ¨¡å‹åˆ—è¡¨
        return {
            "object": "list",
            "data": [
                {
                    "id": model,
                    "object": "model",
                    "created": int(time.time()),
                    "owned_by": provider,
                    "permission": [],
                    "root": model,
                    "parent": None
                }
                for model in ALLOWED_MODELS[provider]
            ]
        }
    else:
        # è¿”å›æ‰€æœ‰æ¨¡å‹
        all_models = []
        for prov, models in ALLOWED_MODELS.items():
            for model in models:
                all_models.append({
                    "id": model,
                    "object": "model",
                    "created": int(time.time()),
                    "owned_by": prov,
                    "permission": [],
                    "root": model,
                    "parent": None
                })
        
        return {"object": "list", "data": all_models}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "version": "2.3.0 (OpenAI Compatible)",
        "features": [
            "Gemini â†’ OpenAI æ ¼å¼è½¬æ¢",
            "æ ‡å‡† SSE æµå¼è¾“å‡º",
            "é€Ÿç‡é™åˆ¶ä¿æŠ¤",
            "è¾“å…¥éªŒè¯"
        ],
        "security": {
            "rate_limit": f"{RATE_LIMIT_REQUESTS} req/{RATE_LIMIT_WINDOW}s",
            "access_token": "enabled" if ENABLE_ACCESS_TOKEN else "disabled"
        }
    }

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ä¿¡æ¯"""
    return {
        "service": "Grok & Gemini API Proxy (OpenAI Compatible)",
        "version": "2.3.0",
        "compatibility": "OpenAI API v1",
        "features": [
            "âœ… Gemini è‡ªåŠ¨è½¬æ¢ä¸º OpenAI æ ¼å¼",
            "âœ… å®Œæ•´æ”¯æŒæµå¼å’Œéæµå¼è¾“å‡º",
            "âœ… æ ‡å‡† SSE (Server-Sent Events) æ ¼å¼",
            "âœ… é€Ÿç‡é™åˆ¶ä¿æŠ¤",
            "âœ… è¯·æ±‚éªŒè¯å’Œå®‰å…¨é˜²æŠ¤"
        ],
        "providers": ["grok", "gemini"],
        "models": ALLOWED_MODELS
    }

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    await http_client.aclose()
    logger.info("æœåŠ¡å…³é—­")

if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
